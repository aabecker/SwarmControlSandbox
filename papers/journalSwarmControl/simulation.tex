
\section{Simulation of Control Laws}\label{sec:simulation}
% Simulation:
%%  Mean control results   (image and plot(s) )
%%  Variance Control      (image and plot(s) )
%%  Hybrid control   (image and plot(s) )

Our simulations use a Javascript port of \href{http://box2d.org/}{Box2D}, a popular 2D physics engine with support for rigid-body dynamics and fixed-time step simulation~\cite{catto2010box2d}.  All experiments ran on a Chrome web browser on a 2.6 GHz Macbook.  \href{https://github.com/aabecker/SwarmControlSandbox/blob/master/exampleControllers/BlockPushingIROS2015.html}{All code is available at}~\cite{Shahrokhi2015}.

\subsection{Controlling the mean position}

We control mean position with a PD controller that uses the mean position and mean velocity. Our control input is the global force applied to each robot:
\begin{align}
u_x &= K_{p}(x_{goal} - \bar{x}) + K_{d}(0-\bar{v}_x) \nonumber\\
u_y &= K_{p}(y_{goal}  - \bar{y}) + K_{d}(0-\bar{v}_y)  \label{eq:PDcontrolPosition}
\end{align}
here $K_{p}$ is the proportional gain, and $K_{d}$ is the derivative gain. We performed a parameter sweep to identify the best values.  Representative experiments are shown in Fig.~\ref{fig:gainvalues}. 100 robots were used and the maximum speed was 3 meters per second. As shown in Fig.~\ref{fig:gainvalues}, we can achieve an overshoot of 1\% and a  rise time of 1.52 s with $K_{p}= 4$, and  $K_{d} = 1$. 
\begin{figure}
\centering
\begin{overpic}[width = 0.7\columnwidth ]{gains.eps}
\end{overpic}
%\begin{overpic}[width =0.49\columnwidth]{meanVariance4.eps}
%\end{overpic}
\vspace{-1em}
\caption{\label{fig:gainvalues} Tuning proportional ($K_p$, top) and derivative ($K_d$, bottom)  gain values in \eqref{eq:PDcontrolPosition} improves performance with $n = 100$ robots. %b) Simulation result with 100 robots under hybrid control Alg.~\ref{alg:MeanVarianceControl}, which  controls both the mean position (top) and variance (bottom). For ease of analysis, only $x$ position and variance are shown.
%\vspace{-2em}
}
\end{figure}

%\begin{figure}
%\centering
%\begin{overpic}[width = \columnwidth* 2/3 ]{meanVideoSnapShot.png}
%\end{overpic}
%\vspace{0 em}
%\caption{\label{fig:meanVideo} A frame from video attachment showing mean position control on a swarm of 200 robots. The mean position of the swarm traces ``SWARM"~\cite{ShivaVideo2015}. 
%%\vspace{-2em}
%}
%\end{figure}


\subsection{Controlling the variance}
\begin{figure}
\centering
\begin{overpic}[width =0.7 \columnwidth] {brownianWpublish.eps}
\end{overpic}
\vspace{-1em}
\caption{\label{fig:varyBrownian} Increased noise results in more responsive variance control because stronger Brownian noise causes a faster increase of variance.
%\vspace{-2em}
}
\end{figure}

%cite the control law, explain experiment (number of robots, maximum speed, ).

For variance control we use the control law discussed in Section~\ref{sec:VarianceControl}.  Moving away from the wall and waiting is sufficient to increase variance because Brownian noise naturally disperses the swarm in such a way that the variance increases linearly~\cite{einstein1956investigations}.  If faster dispersion is needed, the swarm can be pushed through obstacles such as a diffraction grating or Pachinko board~\cite{Becker2013b}. %no need to worry about blocks in this controller.

The variance control law to regulate the variance to $\sigma^2_{ref}$ has three gains:
\begin{align}
u_x &= K_{p}(x_{goal}(\sigma^2_{ref}) - \bar{x}) - K_{d}\bar{v}_x + K_{i}(\sigma^2_{ref}-\sigma^2_{x}) \nonumber\\
u_y &= K_{p}(y_{goal}(\sigma^2_{ref})  - \bar{y}) - K_{d}\bar{v}_y + K_{i}(\sigma^2_{ref}-\sigma^2_{y}).  \label{eq:PDcontrolVariance}
\end{align}
In a slight abuse of notation we call the gain scaling the variance error $K_i$ because the variance, if unregulated, integrates over time.
Eq.~\eqref{eq:PDcontrolVariance} assumes the nearest wall is to the left of the robot at $x=0$, and chooses a reference goal position that in steady-state would have the correct variance according to \eqref{eq:VarianceUniformDistribution}:
\begin{align}
x_{goal}(\sigma^2_{ref}) = r + \sqrt{3\sigma^2_{ref}}
\end{align}
 If another wall is closer, the signs of $[K_p,K_i]$ are inverted, and the location $x_{goal}$ is translated.  Results are shown in Fig.~\ref{fig:varyBrownian}, with $K_{p,i,d} = [4,1,1]$.



%\todo{image showing control x variance and y-variance out of phase}


\subsection{Hybrid Control of mean and variance}

Fig. ~\ref{fig:hybrid} shows a simulation run of the hybrid controller in Alg.~\ref{alg:MeanVarianceControl} with 100 robots in a square workspace containing no internal obstacles. 
%\todo{plot showing 1.5 cycles of mean position, and a variance goal.  We might need a longer time}
\begin{figure}
\centering
\begin{overpic}[width =0.7\columnwidth]{meanVariance4.eps}
\end{overpic}
\vspace{-1em}
\caption{\label{fig:hybrid} Simulation result with 100 robots under hybrid control Alg.~\ref{alg:MeanVarianceControl}, which  controls both the mean position (top) and variance (bottom). For ease of analysis, only $x$ position and variance are shown.
\vspace{-2em}
}
\end{figure}

%\begin{figure}
%\centering
%\begin{overpic}[width = \columnwidth * 2/3]{VideoSnapShot.png}
%\end{overpic}
%\vspace{-1em}
%\caption{\label{fig:videoVar} A frame from video, using Alg.~\ref{alg:MeanVarianceControl} to control variance and mean position of a swarm of 200 robots~\cite{ShivaVideo2015}.
%%\vspace{-2em}
%}
%\end{figure}






